{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# 1. CONFIGURATION\n",
    "###############################################################################\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "DATA_PATH = (\n",
    "    \"/mnt/c/Users/mikig/Desktop/UPC/PAE/Datasets/\"\n",
    "    \"9810e03bba4983da_MOHANAD_A4706/\"\n",
    "    \"9810e03bba4983da_MOHANAD_A4706/data/NF-UQ-NIDS-v2.csv\"\n",
    ")\n",
    "\n",
    "TARGET_COLUMN = \"Attack\"\n",
    "DROP_COLUMNS = [\"Dataset\", \"Label\"]\n",
    "\n",
    "# If EPOCHS=0 => use large epochs + early stopping\n",
    "# If EPOCHS!=0 => train for exactly EPOCHS\n",
    "EPOCHS = 0  # Toggle here. 0 => Early stopping, >0 => fixed epochs.\n",
    "\n",
    "SCALER_CHUNKSIZE = 100_000\n",
    "MAX_SCALER_CHUNKS = 5\n",
    "\n",
    "CHUNK_SIZE = 1024  # Good default chunk size for memory/performance\n",
    "TRAIN_SPLIT_RATIO = 0.8\n",
    "CLAMP_VALUE = 1e9\n",
    "CHUNK_SHUFFLE_BUFFER = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_all_labels(csv_path, target_column=TARGET_COLUMN, chunksize=200000):\n",
    "    \"\"\"\n",
    "    Reads only the target column in chunks. Gathers every unique label in a set.\n",
    "    Ensures no unseen labels at training time.\n",
    "    \"\"\"\n",
    "    print(f\"[INFO] Collecting all unique labels from '{target_column}'...\")\n",
    "    label_set = set()\n",
    "    chunk_idx = 0\n",
    "    for chunk in pd.read_csv(csv_path, usecols=[target_column], chunksize=chunksize):\n",
    "        chunk_idx += 1\n",
    "        chunk.dropna(subset=[target_column], inplace=True)\n",
    "        label_set.update(chunk[target_column].unique())\n",
    "        if chunk_idx % 10 == 0:\n",
    "            print(\n",
    "                f\"  Processed {chunk_idx} label-only chunks... (unique labels so far={len(label_set)})\"\n",
    "            )\n",
    "    all_labels = sorted(label_set)\n",
    "    print(f\"[INFO] Found {len(all_labels)} unique labels total.\")\n",
    "    return all_labels\n",
    "\n",
    "\n",
    "def compute_class_distribution(csv_path, target_col=TARGET_COLUMN, chunksize=100000):\n",
    "    \"\"\"\n",
    "    Reads only 'target_col' in chunks to compute frequency counts for class weighting.\n",
    "    \"\"\"\n",
    "    print(\"[INFO] Computing class distribution (for class weights)...\")\n",
    "    counter = Counter()\n",
    "    cidx = 0\n",
    "    for chunk in pd.read_csv(csv_path, usecols=[target_col], chunksize=chunksize):\n",
    "        cidx += 1\n",
    "        chunk.dropna(subset=[target_col], inplace=True)\n",
    "        counter.update(chunk[target_col].values)\n",
    "        if cidx % 10 == 0:\n",
    "            print(f\"  Processed {cidx} distribution chunks.\")\n",
    "    return counter\n",
    "\n",
    "\n",
    "def make_class_weights(label_encoder, class_counter):\n",
    "    \"\"\"\n",
    "    Creates a dictionary for Keras 'class_weight' from the frequency counts.\n",
    "    Typically: weight = total_samples / (num_classes * class_count).\n",
    "    \"\"\"\n",
    "    total_samples = sum(class_counter.values())\n",
    "    n_classes = len(label_encoder.classes_)\n",
    "    cw = {}\n",
    "    for i, cls_label in enumerate(label_encoder.classes_):\n",
    "        cnt = class_counter.get(cls_label, 0)\n",
    "        if cnt == 0:\n",
    "            cw[i] = 0.0\n",
    "        else:\n",
    "            cw[i] = total_samples / (n_classes * cnt)\n",
    "    return cw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_fit_scaler(\n",
    "    csv_path,\n",
    "    target_column=TARGET_COLUMN,\n",
    "    drop_columns=DROP_COLUMNS,\n",
    "    clamp_value=CLAMP_VALUE,\n",
    "    scaler_chunksize=SCALER_CHUNKSIZE,\n",
    "    max_chunks=MAX_SCALER_CHUNKS,\n",
    "):\n",
    "    \"\"\"\n",
    "    Reads the CSV in moderate chunks to partial_fit a StandardScaler\n",
    "    without loading entire dataset into memory.\n",
    "    \"\"\"\n",
    "    print(\"[INFO] Incrementally partial-fitting a StandardScaler...\")\n",
    "    scaler = StandardScaler()\n",
    "    reader = pd.read_csv(csv_path, chunksize=scaler_chunksize, low_memory=True)\n",
    "\n",
    "    chunk_index = 0\n",
    "    for chunk in reader:\n",
    "        chunk_index += 1\n",
    "        if chunk_index > max_chunks:\n",
    "            break\n",
    "\n",
    "        if drop_columns:\n",
    "            chunk.drop(\n",
    "                columns=[c for c in drop_columns if c in chunk.columns],\n",
    "                inplace=True,\n",
    "                errors=\"ignore\",\n",
    "            )\n",
    "        if target_column in chunk.columns:\n",
    "            chunk.drop(columns=[target_column], inplace=True, errors=\"ignore\")\n",
    "\n",
    "        for col in chunk.columns:\n",
    "            if chunk[col].dtype == object:\n",
    "                chunk[col] = chunk[col].apply(\n",
    "                    lambda x: hash(x) % (2**31) if pd.notna(x) else 0\n",
    "                )\n",
    "\n",
    "        numeric_chunk = chunk.select_dtypes(include=[np.number]).astype(np.float64)\n",
    "        numeric_chunk.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        numeric_chunk = numeric_chunk.clip(-clamp_value, clamp_value)\n",
    "        numeric_chunk.fillna(0, inplace=True)\n",
    "\n",
    "        scaler.partial_fit(numeric_chunk)\n",
    "        print(\n",
    "            f\"  [DEBUG] partial_fit chunk #{chunk_index}, shape={numeric_chunk.shape}\"\n",
    "        )\n",
    "\n",
    "    print(\"[INFO] Done partial-fitting the StandardScaler.\")\n",
    "    return scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_chunks(csv_path, chunk_size=CHUNK_SIZE, train_ratio=TRAIN_SPLIT_RATIO):\n",
    "    \"\"\"\n",
    "    Calculates the number of chunks based on total samples and chunk size.\n",
    "    Splits them by index for train/val.\n",
    "    \"\"\"\n",
    "    # Define total number of samples (you can adjust this if known)\n",
    "    total_samples = 75987975  # Replace with your actual total samples if different\n",
    "\n",
    "    # Calculate the number of training and validation samples\n",
    "    train_samples = int(total_samples * train_ratio)\n",
    "    validation_samples = total_samples - train_samples\n",
    "\n",
    "    # Calculate number of training and validation chunks\n",
    "    train_steps = round((train_samples / chunk_size), 0)\n",
    "    train_chunks = int(\n",
    "        train_steps if train_samples / chunk_size < train_steps else train_steps + 1\n",
    "    )\n",
    "    validation_steps = round((validation_samples / chunk_size), 0)\n",
    "    val_chunks = int(\n",
    "        validation_steps\n",
    "        if validation_samples / chunk_size < validation_steps\n",
    "        else validation_steps + 1\n",
    "    )\n",
    "\n",
    "    print(f\"Total samples: {total_samples}\")\n",
    "    print(f\"Training samples: {train_samples}\")\n",
    "    print(f\"Validation samples: {validation_samples}\")\n",
    "    print(f\"Training chunks: {train_chunks}\")\n",
    "    print(f\"Validation chunks: {val_chunks}\")\n",
    "\n",
    "    return train_chunks, val_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_generator(\n",
    "    csv_path,\n",
    "    chunk_size,\n",
    "    target_column,\n",
    "    drop_columns,\n",
    "    scaler,\n",
    "    label_encoder,\n",
    "    clamp_value,\n",
    "    shuffle_seed,\n",
    "    start_chunk,\n",
    "    end_chunk,\n",
    "    is_val=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generator that yields (X, y) for chunk indices [start_chunk, end_chunk).\n",
    "    \"\"\"\n",
    "    reader = pd.read_csv(csv_path, chunksize=chunk_size, low_memory=True)\n",
    "    rng = np.random.default_rng(shuffle_seed)\n",
    "\n",
    "    cidx = 0\n",
    "    for chunk in reader:\n",
    "        if cidx < start_chunk:\n",
    "            cidx += 1\n",
    "            continue\n",
    "        if cidx >= end_chunk:\n",
    "            break\n",
    "\n",
    "        # Drop columns not needed\n",
    "        if drop_columns:\n",
    "            chunk.drop(\n",
    "                columns=[c for c in drop_columns if c in chunk.columns],\n",
    "                inplace=True,\n",
    "                errors=\"ignore\",\n",
    "            )\n",
    "\n",
    "        # Drop rows with missing target\n",
    "        chunk.dropna(subset=[target_column], inplace=True)\n",
    "        if chunk.empty:\n",
    "            cidx += 1\n",
    "            continue\n",
    "\n",
    "        labels = chunk.pop(target_column)\n",
    "\n",
    "        # Convert object columns -> numeric\n",
    "        for col in chunk.columns:\n",
    "            if chunk[col].dtype == object:\n",
    "                chunk[col] = chunk[col].apply(\n",
    "                    lambda x: hash(x) % (2**31) if pd.notna(x) else 0\n",
    "                )\n",
    "\n",
    "        X_chunk = chunk.select_dtypes(include=[np.number]).astype(np.float64)\n",
    "        X_chunk.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        X_chunk = X_chunk.clip(-clamp_value, clamp_value)\n",
    "        X_chunk.fillna(0, inplace=True)\n",
    "\n",
    "        # Scale\n",
    "        if scaler is not None:\n",
    "            X_chunk = scaler.transform(X_chunk)\n",
    "        X_chunk = X_chunk.astype(np.float32)\n",
    "\n",
    "        y_encoded = label_encoder.transform(labels)\n",
    "\n",
    "        # Shuffle rows if training\n",
    "        if not is_val:\n",
    "            idx = np.arange(len(X_chunk))\n",
    "            rng.shuffle(idx)\n",
    "            X_chunk = X_chunk[idx]\n",
    "            y_encoded = y_encoded[idx]\n",
    "\n",
    "        yield X_chunk, y_encoded\n",
    "        cidx += 1\n",
    "\n",
    "\n",
    "def create_dataset(\n",
    "    csv_path,\n",
    "    chunk_size,\n",
    "    target_column,\n",
    "    drop_columns,\n",
    "    scaler,\n",
    "    label_encoder,\n",
    "    clamp_value,\n",
    "    shuffle_seed,\n",
    "    start_chunk,\n",
    "    end_chunk,\n",
    "    is_val=False,\n",
    "    repeat_epochs=1,\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a tf.data.Dataset from chunk_generator for [start_chunk, end_chunk).\n",
    "    If repeat_epochs>0, we do ds.repeat(repeat_epochs).\n",
    "    If repeat_epochs=0, we skip ds.repeat => pass is driven by model.fit epochs.\n",
    "    \"\"\"\n",
    "    print(\n",
    "        f\"[INFO] Building dataset for chunks [{start_chunk}, {end_chunk}) (is_val={is_val}), repeat={repeat_epochs}\"\n",
    "    )\n",
    "    output_types = (tf.float32, tf.int32)\n",
    "    output_shapes = (tf.TensorShape([None, None]), tf.TensorShape([None]))\n",
    "\n",
    "    ds = tf.data.Dataset.from_generator(\n",
    "        lambda: chunk_generator(\n",
    "            csv_path=csv_path,\n",
    "            chunk_size=chunk_size,\n",
    "            target_column=target_column,\n",
    "            drop_columns=drop_columns,\n",
    "            scaler=scaler,\n",
    "            label_encoder=label_encoder,\n",
    "            clamp_value=clamp_value,\n",
    "            shuffle_seed=shuffle_seed,\n",
    "            start_chunk=start_chunk,\n",
    "            end_chunk=end_chunk,\n",
    "            is_val=is_val,\n",
    "        ),\n",
    "        output_types=output_types,\n",
    "        output_shapes=output_shapes,\n",
    "    )\n",
    "\n",
    "    if repeat_epochs > 0:\n",
    "        ds = ds.repeat(repeat_epochs)\n",
    "\n",
    "    if not is_val:\n",
    "        ds = ds.shuffle(buffer_size=CHUNK_SHUFFLE_BUFFER, reshuffle_each_iteration=True)\n",
    "\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_dim, num_classes):\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            keras.layers.InputLayer(input_shape=(input_dim,)),\n",
    "            keras.layers.Dense(256, activation=\"relu\"),\n",
    "            keras.layers.Dropout(0.3),\n",
    "            keras.layers.Dense(128, activation=\"relu\"),\n",
    "            keras.layers.Dropout(0.3),\n",
    "            keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(1e-3),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves(history):\n",
    "    acc = history.history.get(\"accuracy\", [])\n",
    "    val_acc = history.history.get(\"val_accuracy\", [])\n",
    "    loss = history.history.get(\"loss\", [])\n",
    "    val_loss = history.history.get(\"val_loss\", [])\n",
    "\n",
    "    epochs_range = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, acc, label=\"Train Acc\")\n",
    "    plt.plot(epochs_range, val_acc, label=\"Val Acc\")\n",
    "    plt.title(\"Accuracy Over Epochs\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, loss, label=\"Train Loss\")\n",
    "    plt.plot(epochs_range, val_loss, label=\"Val Loss\")\n",
    "    plt.title(\"Loss Over Epochs\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, labels):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        cm, annot=False, cmap=\"Blues\", xticklabels=labels, yticklabels=labels, fmt=\"d\"\n",
    "    )\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # 1) Collect all unique labels => fit LabelEncoder\n",
    "    all_labels = collect_all_labels(\n",
    "        DATA_PATH, target_column=TARGET_COLUMN, chunksize=100_000\n",
    "    )\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(all_labels)\n",
    "    num_classes = len(all_labels)\n",
    "    print(f\"[INFO] LabelEncoder fitted on {num_classes} classes.\")\n",
    "\n",
    "    # 2) Partial-fit StandardScaler\n",
    "    scaler = partial_fit_scaler(\n",
    "        csv_path=DATA_PATH,\n",
    "        target_column=TARGET_COLUMN,\n",
    "        drop_columns=DROP_COLUMNS,\n",
    "        clamp_value=CLAMP_VALUE,\n",
    "        scaler_chunksize=SCALER_CHUNKSIZE,\n",
    "        max_chunks=MAX_SCALER_CHUNKS,\n",
    "    )\n",
    "\n",
    "    # 3) Class distribution => class weights\n",
    "    class_counter = compute_class_distribution(\n",
    "        DATA_PATH, TARGET_COLUMN, chunksize=100_000\n",
    "    )\n",
    "    class_weight_dict = make_class_weights(label_encoder, class_counter)\n",
    "    print(\"[INFO] Class weights:\")\n",
    "    for idx, w in class_weight_dict.items():\n",
    "        print(f\"  index={idx}, label={label_encoder.classes_[idx]}, weight={w:.3f}\")\n",
    "\n",
    "    # 4) Count total chunks => train/val split\n",
    "    train_chunks, val_chunks = count_chunks(\n",
    "        DATA_PATH, chunk_size=CHUNK_SIZE, train_ratio=TRAIN_SPLIT_RATIO\n",
    "    )\n",
    "\n",
    "    # 5) Decide how many epochs (0 => large epoch + early stopping, else fixed)\n",
    "    if EPOCHS == 0:\n",
    "        print(\"[INFO] EPOCHS=0 => Using early stopping with a maximum of 50 epochs.\")\n",
    "        epochs_for_fit = 50\n",
    "        callbacks_list = [\n",
    "            keras.callbacks.EarlyStopping(\n",
    "                monitor=\"val_loss\", patience=3, restore_best_weights=True\n",
    "            )\n",
    "        ]\n",
    "        repeat_for_dataset = 0  # single pass each epoch\n",
    "    else:\n",
    "        print(\n",
    "            f\"[INFO] EPOCHS={EPOCHS} => Using exactly {EPOCHS} epochs (no early stopping).\"\n",
    "        )\n",
    "        epochs_for_fit = EPOCHS\n",
    "        callbacks_list = []\n",
    "        repeat_for_dataset = EPOCHS  # dataset repeats EPOCHS times\n",
    "\n",
    "    # 6) Create train & val datasets\n",
    "    train_ds = create_dataset(\n",
    "        csv_path=DATA_PATH,\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        target_column=TARGET_COLUMN,\n",
    "        drop_columns=DROP_COLUMNS,\n",
    "        scaler=scaler,\n",
    "        label_encoder=label_encoder,\n",
    "        clamp_value=CLAMP_VALUE,\n",
    "        shuffle_seed=SEED,\n",
    "        start_chunk=0,\n",
    "        end_chunk=train_chunks,\n",
    "        is_val=False,\n",
    "        repeat_epochs=repeat_for_dataset,\n",
    "    )\n",
    "\n",
    "    val_ds = create_dataset(\n",
    "        csv_path=DATA_PATH,\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        target_column=TARGET_COLUMN,\n",
    "        drop_columns=DROP_COLUMNS,\n",
    "        scaler=scaler,\n",
    "        label_encoder=label_encoder,\n",
    "        clamp_value=CLAMP_VALUE,\n",
    "        shuffle_seed=SEED,\n",
    "        start_chunk=train_chunks,\n",
    "        end_chunk=train_chunks + val_chunks,\n",
    "        is_val=True,\n",
    "        repeat_epochs=repeat_for_dataset,\n",
    "    )\n",
    "\n",
    "    # 7) Determine input_dim from the first training batch\n",
    "    for Xb, yb in train_ds.take(1):\n",
    "        input_dim = Xb.shape[1]\n",
    "        print(f\"[INFO] Detected input_dim={input_dim} from the first training batch.\")\n",
    "        break\n",
    "\n",
    "    # 8) Build the model\n",
    "    model = build_model(input_dim, num_classes)\n",
    "    model.summary()\n",
    "\n",
    "    # 9) Train\n",
    "    print(\"[INFO] Starting training now...\")\n",
    "    history = model.fit(\n",
    "        train_ds.repeat(),\n",
    "        epochs=epochs_for_fit,\n",
    "        validation_data=val_ds,\n",
    "        class_weight=class_weight_dict,\n",
    "        callbacks=callbacks_list,\n",
    "    )\n",
    "    print(\"[INFO] Training complete.\")\n",
    "\n",
    "    # 10) Plot training curves\n",
    "    plot_training_curves(history)\n",
    "\n",
    "    # 11) Evaluate on validation data fully\n",
    "    print(\"[INFO] Evaluating on validation data to build confusion matrix ...\")\n",
    "    all_preds = []\n",
    "    all_labels_eval = []\n",
    "    for X_val, y_val in val_ds:\n",
    "        preds = model.predict(X_val)\n",
    "        preds_class = np.argmax(preds, axis=1)\n",
    "        all_preds.extend(preds_class)\n",
    "        all_labels_eval.extend(y_val.numpy())\n",
    "\n",
    "    cm = confusion_matrix(all_labels_eval, all_preds)\n",
    "    print(\"\\n[INFO] Confusion Matrix:\\n\", cm)\n",
    "    plot_confusion_matrix(cm, labels=label_encoder.classes_)\n",
    "\n",
    "    print(\"\\n[INFO] Classification Report:\")\n",
    "    print(\n",
    "        classification_report(\n",
    "            all_labels_eval,\n",
    "            all_preds,\n",
    "            target_names=label_encoder.classes_,\n",
    "            zero_division=0,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # 12) Save model\n",
    "    model.save(\"netflow_classification_model_conditional_epochs.keras\")\n",
    "    print(\n",
    "        \"[INFO] Model saved to 'netflow_classification_model_conditional_epochs.keras'.\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
