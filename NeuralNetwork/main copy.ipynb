{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.18.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Normalization,\n",
    "    StringLookup,\n",
    "    IntegerLookup,\n",
    "    Embedding,\n",
    "    Flatten,\n",
    "    Concatenate,\n",
    "    Dense,\n",
    "    BatchNormalization,\n",
    "    Dropout,\n",
    "    Input,\n",
    ")\n",
    "from tensorflow.keras import Model\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Ensure TensorFlow version compatibility\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Configuration Parameters\n",
    "\n",
    "# File path to the CSV dataset\n",
    "file_path = \"/mnt/c/Users/mikig/Desktop/UPC/PAE/Datasets/9810e03bba4983da_MOHANAD_A4706/9810e03bba4983da_MOHANAD_A4706/data/NF-UQ-NIDS-v2.csv\"\n",
    "\n",
    "# Batch size for training\n",
    "batch_size = 64\n",
    "\n",
    "# Number of batches to use for adapting normalization and lookup layers\n",
    "batches_for_adaptation = 5000\n",
    "\n",
    "# Validation split (20% for validation)\n",
    "validation_split = 0.2\n",
    "\n",
    "# Random seed for reproducibility\n",
    "seed = 42\n",
    "\n",
    "# Early stopping and model checkpointing parameters\n",
    "patience = 3  # For EarlyStopping\n",
    "model_checkpoint_path = \"best_model.keras\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Define Column Defaults\n",
    "\n",
    "# Define column defaults with appropriate data types\n",
    "column_defaults = [\n",
    "    tf.string,  # 0: IPV4_SRC_ADDR\n",
    "    tf.int64,  # 1: L4_SRC_PORT\n",
    "    tf.string,  # 2: IPV4_DST_ADDR\n",
    "    tf.int64,  # 3: L4_DST_PORT\n",
    "    tf.int64,  # 4: PROTOCOL\n",
    "    tf.float32,  # 5: L7_PROTO\n",
    "    tf.int64,  # 6: IN_BYTES\n",
    "    tf.int64,  # 7: IN_PKTS\n",
    "    tf.int64,  # 8: OUT_BYTES\n",
    "    tf.int64,  # 9: OUT_PKTS\n",
    "    tf.int64,  # 10: TCP_FLAGS\n",
    "    tf.int64,  # 11: CLIENT_TCP_FLAGS\n",
    "    tf.int64,  # 12: SERVER_TCP_FLAGS\n",
    "    tf.int64,  # 13: FLOW_DURATION_MILLISECONDS\n",
    "    tf.int64,  # 14: DURATION_IN\n",
    "    tf.int64,  # 15: DURATION_OUT\n",
    "    tf.int64,  # 16: MIN_TTL\n",
    "    tf.int64,  # 17: MAX_TTL\n",
    "    tf.int64,  # 18: LONGEST_FLOW_PKT\n",
    "    tf.int64,  # 19: SHORTEST_FLOW_PKT\n",
    "    tf.int64,  # 20: MIN_IP_PKT_LEN\n",
    "    tf.int64,  # 21: MAX_IP_PKT_LEN\n",
    "    tf.float32,  # 22: SRC_TO_DST_SECOND_BYTES\n",
    "    tf.float32,  # 23: DST_TO_SRC_SECOND_BYTES\n",
    "    tf.int64,  # 24: RETRANSMITTED_IN_BYTES\n",
    "    tf.int64,  # 25: RETRANSMITTED_IN_PKTS\n",
    "    tf.int64,  # 26: RETRANSMITTED_OUT_BYTES\n",
    "    tf.int64,  # 27: RETRANSMITTED_OUT_PKTS\n",
    "    tf.float32,  # 28: SRC_TO_DST_AVG_THROUGHPUT\n",
    "    tf.float32,  # 29: DST_TO_SRC_AVG_THROUGHPUT\n",
    "    tf.int64,  # 30: NUM_PKTS_UP_TO_128_BYTES\n",
    "    tf.int64,  # 31: NUM_PKTS_128_TO_256_BYTES\n",
    "    tf.int64,  # 32: NUM_PKTS_256_TO_512_BYTES\n",
    "    tf.int64,  # 33: NUM_PKTS_512_TO_1024_BYTES\n",
    "    tf.int64,  # 34: NUM_PKTS_1024_TO_1514_BYTES\n",
    "    tf.int64,  # 35: TCP_WIN_MAX_IN\n",
    "    tf.int64,  # 36: TCP_WIN_MAX_OUT\n",
    "    tf.int64,  # 37: ICMP_TYPE\n",
    "    tf.int64,  # 38: ICMP_IPV4_TYPE\n",
    "    tf.int64,  # 39: DNS_QUERY_ID\n",
    "    tf.int64,  # 40: DNS_QUERY_TYPE\n",
    "    tf.int64,  # 41: DNS_TTL_ANSWER\n",
    "    tf.float32,  # 42: FTP_COMMAND_RET_CODE\n",
    "    tf.int64,  # 43: Label\n",
    "    tf.string,  # 44: Attack\n",
    "    tf.string,  # 45: Dataset\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Feature Categorization\n",
    "\n",
    "# Define feature categories\n",
    "numerical_features = [\n",
    "    \"IN_BYTES\",\n",
    "    \"OUT_BYTES\",\n",
    "    \"RETRANSMITTED_IN_BYTES\",\n",
    "    \"RETRANSMITTED_OUT_BYTES\",\n",
    "    \"IN_PKTS\",\n",
    "    \"OUT_PKTS\",\n",
    "    \"RETRANSMITTED_IN_PKTS\",\n",
    "    \"RETRANSMITTED_OUT_PKTS\",\n",
    "    \"FLOW_DURATION_MILLISECONDS\",\n",
    "    \"DURATION_IN\",\n",
    "    \"DURATION_OUT\",\n",
    "    \"SRC_TO_DST_SECOND_BYTES\",\n",
    "    \"DST_TO_SRC_SECOND_BYTES\",\n",
    "    \"SRC_TO_DST_AVG_THROUGHPUT\",\n",
    "    \"DST_TO_SRC_AVG_THROUGHPUT\",\n",
    "    \"MIN_IP_PKT_LEN\",\n",
    "    \"MAX_IP_PKT_LEN\",\n",
    "    \"LONGEST_FLOW_PKT\",\n",
    "    \"SHORTEST_FLOW_PKT\",\n",
    "    \"TCP_WIN_MAX_IN\",\n",
    "    \"TCP_WIN_MAX_OUT\",\n",
    "    \"FTP_COMMAND_RET_CODE\",\n",
    "]\n",
    "\n",
    "categorical_features = [\"PROTOCOL\", \"ICMP_TYPE\", \"ICMP_IPV4_TYPE\", \"DNS_QUERY_TYPE\"]\n",
    "\n",
    "identifier_features = [\"IPV4_SRC_ADDR\", \"IPV4_DST_ADDR\", \"L4_SRC_PORT\", \"L4_DST_PORT\"]\n",
    "\n",
    "columns_to_remove = [\"Label\", \"Dataset\"]\n",
    "\n",
    "label_column = \"Attack\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 75987975\n",
      "Training samples: 60790380\n",
      "Validation samples: 15197595\n",
      "Training steps: 949850\n",
      "Validation steps: 237463\n",
      "Training and validation datasets created successfully using take and skip.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Create Training and Validation Datasets (Using Take and Skip)\n",
    "\n",
    "# Define total number of samples (you can adjust this if known)\n",
    "total_samples = 75987975  # Replace with your actual total samples if different\n",
    "\n",
    "# Calculate the number of training and validation samples\n",
    "train_samples = int(total_samples * 0.8)\n",
    "validation_samples = total_samples - train_samples\n",
    "\n",
    "# Calculate number of training and validation steps\n",
    "train_steps = round((train_samples / batch_size), 0)\n",
    "train_steps = int(\n",
    "    train_steps if train_samples / batch_size < train_steps else train_steps + 1\n",
    ")\n",
    "validation_steps = round((validation_samples / batch_size), 0)\n",
    "validation_steps = int(\n",
    "    validation_steps\n",
    "    if validation_samples / batch_size < validation_steps\n",
    "    else validation_steps + 1\n",
    ")\n",
    "\n",
    "print(f\"Total samples: {total_samples}\")\n",
    "print(f\"Training samples: {train_samples}\")\n",
    "print(f\"Validation samples: {validation_samples}\")\n",
    "print(f\"Training steps: {train_steps}\")\n",
    "print(f\"Validation steps: {validation_steps}\")\n",
    "\n",
    "# Create a single dataset without validation_split and subset\n",
    "full_dataset = tf.data.experimental.make_csv_dataset(\n",
    "    file_path,\n",
    "    label_name=label_column,\n",
    "    batch_size=batch_size,\n",
    "    column_defaults=column_defaults,\n",
    "    shuffle=True,  # Shuffle the data\n",
    "    num_epochs=1,  # Repeat once\n",
    "    ignore_errors=False,\n",
    "    shuffle_seed=seed,  # Use 'shuffle_seed' instead of 'seed'\n",
    ")\n",
    "\n",
    "# Split the dataset using take and skip\n",
    "train_dataset = full_dataset.take(train_samples // batch_size).prefetch(\n",
    "    tf.data.AUTOTUNE\n",
    ")\n",
    "validation_dataset = full_dataset.skip(train_samples // batch_size).prefetch(\n",
    "    tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "print(\"Training and validation datasets created successfully using take and skip.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Label Encoding\n",
    "\n",
    "# Create a StringLookup layer to encode string labels into integers\n",
    "label_lookup = StringLookup(output_mode=\"int\")\n",
    "\n",
    "print(\"Constructing vocabulary for labels...\")\n",
    "# Adapt the label_lookup layer to the 'Attack' column\n",
    "attack_column_data = train_dataset.map(lambda features, label: label).take(\n",
    "    batches_for_adaptation\n",
    ")\n",
    "label_lookup.adapt(attack_column_data)\n",
    "\n",
    "# After adapting the label_lookup layer\n",
    "vocab = label_lookup.get_vocabulary()\n",
    "print(\"Vocabulary size:\", len(vocab))\n",
    "print(\"Vocabulary:\", vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Normalization of Numerical Features\n",
    "\n",
    "# Create and adapt Normalization layers for each numerical feature\n",
    "normalizers = {}\n",
    "for feature in numerical_features:\n",
    "    normalizer = Normalization(\n",
    "        axis=None\n",
    "    )  # axis=None normalizes over the entire feature\n",
    "    # Extract the feature data for adaptation\n",
    "    feature_data = (\n",
    "        train_dataset.map(\n",
    "            lambda features, label: tf.expand_dims(features[feature], axis=-1)\n",
    "        )\n",
    "        .unbatch()\n",
    "        .take(batches_for_adaptation)\n",
    "    )\n",
    "    # Adapt the normalizer with feature values\n",
    "    normalizer.adapt(feature_data)\n",
    "    normalizers[feature] = normalizer\n",
    "    print(f\"Normalization for {feature} adapted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Categorical Feature Encoding\n",
    "\n",
    "from tensorflow.keras.layers import IntegerLookup, Embedding\n",
    "\n",
    "# Define lookup layers for categorical features\n",
    "categorical_lookup_layers = {}\n",
    "embedding_layers = {}\n",
    "embedding_dim = 8  # You can adjust this based on experimentation\n",
    "\n",
    "for feature in categorical_features:\n",
    "    # Create and adapt IntegerLookup layers\n",
    "    lookup = IntegerLookup(output_mode=\"int\", mask_token=None)\n",
    "    # Extract the feature data for adaptation\n",
    "    feature_data = train_dataset.map(lambda features, label: features[feature]).take(\n",
    "        batches_for_adaptation\n",
    "    )\n",
    "    lookup.adapt(feature_data)\n",
    "    categorical_lookup_layers[feature] = lookup\n",
    "    print(f\"Lookup layer for {feature} adapted.\")\n",
    "\n",
    "    # Create Embedding layers\n",
    "    vocab_size = len(lookup.get_vocabulary()) + 1  # +1 for masking if necessary\n",
    "    embedding = Embedding(\n",
    "        input_dim=vocab_size, output_dim=embedding_dim, name=f\"{feature}_embedding\"\n",
    "    )\n",
    "    embedding_layers[feature] = embedding\n",
    "    print(f\"Embedding layer for {feature} created with vocab size {vocab_size}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-adapt Normalization for `bytes_per_packet`\n",
    "bytes_per_packet_normalizer = Normalization(axis=None)\n",
    "bytes_pp_data = (\n",
    "    train_dataset.map(\n",
    "        lambda features, label: tf.cast(features[\"IN_BYTES\"], tf.float32)\n",
    "        / (tf.cast(features[\"IN_PKTS\"], tf.float32) + 1e-6)\n",
    "    )\n",
    "    .unbatch()\n",
    "    .take(batches_for_adaptation)\n",
    ")\n",
    "\n",
    "# Adapt normalizer\n",
    "bytes_per_packet_normalizer.adapt(bytes_pp_data)\n",
    "normalizers[\"bytes_per_packet\"] = bytes_per_packet_normalizer\n",
    "print(\"Normalization for 'bytes_per_packet' pre-adapted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(features, label):\n",
    "    def ip_to_int(ip_string):\n",
    "        try:\n",
    "            octets = tf.strings.split(ip_string, \".\")\n",
    "            octets = tf.strings.to_number(octets, out_type=tf.int64)\n",
    "            ip_int = (\n",
    "                tf.bitwise.left_shift(octets[:, 0], 24)\n",
    "                + tf.bitwise.left_shift(octets[:, 1], 16)\n",
    "                + tf.bitwise.left_shift(octets[:, 2], 8)\n",
    "                + octets[:, 3]\n",
    "            )\n",
    "            return ip_int\n",
    "        except Exception:\n",
    "            return tf.zeros_like(ip_string, dtype=tf.int64)\n",
    "\n",
    "    # Remove unnecessary columns\n",
    "    for col in columns_to_remove:\n",
    "        features.pop(col, None)\n",
    "\n",
    "    # Convert IP columns to integers and ensure correct batch shape\n",
    "    for col in identifier_features[:2]:  # IPV4_SRC_ADDR, IPV4_DST_ADDR\n",
    "        features[col] = ip_to_int(features[col])  # Ensure batch shape (64,)\n",
    "\n",
    "    # Normalize numerical features\n",
    "    for feature in numerical_features:\n",
    "        if feature in features:\n",
    "            features[feature] = tf.squeeze(\n",
    "                normalizers[feature](\n",
    "                    tf.expand_dims(tf.cast(features[feature], tf.float32), axis=-1)\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            features[feature] = 0.0  # Default value for missing features\n",
    "\n",
    "    # Compute and normalize `bytes_per_packet`\n",
    "    if \"IN_BYTES\" in features and \"IN_PKTS\" in features:\n",
    "        bytes_per_packet = tf.cast(features[\"IN_BYTES\"], tf.float32) / (\n",
    "            tf.cast(features[\"IN_PKTS\"], tf.float32) + 1e-6\n",
    "        )\n",
    "        features[\"bytes_per_packet\"] = tf.squeeze(\n",
    "            normalizers[\"bytes_per_packet\"](tf.expand_dims(bytes_per_packet, axis=-1))\n",
    "        )\n",
    "    else:\n",
    "        features[\"bytes_per_packet\"] = 0.0\n",
    "\n",
    "    # Encode categorical features\n",
    "    for feature in categorical_features:\n",
    "        if feature in features:\n",
    "            features[feature] = categorical_lookup_layers[feature](features[feature])\n",
    "        else:\n",
    "            features[feature] = 0  # Default for missing categorical features\n",
    "\n",
    "    for feature in features:\n",
    "        features[feature] = tf.expand_dims(features[feature], axis=-1)\n",
    "\n",
    "    # Create input dictionary\n",
    "    input_dict = {\n",
    "        key: features[key]\n",
    "        for key in features\n",
    "        if key\n",
    "        in numerical_features\n",
    "        + identifier_features\n",
    "        + categorical_features\n",
    "        + [\"bytes_per_packet\"]\n",
    "    }\n",
    "\n",
    "    # Encode the label\n",
    "    label = label_lookup(label)\n",
    "\n",
    "    return input_dict, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Apply Preprocessing to Datasets\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "validation_dataset = validation_dataset.map(\n",
    "    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",
    ").prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Inputs for each feature type\n",
    "numerical_inputs = [\n",
    "    Input(shape=(1,), name=feature, dtype=tf.float32)\n",
    "    for feature in numerical_features + [\"bytes_per_packet\"]\n",
    "]\n",
    "\n",
    "identifier_inputs = [\n",
    "    Input(shape=(1,), name=feature, dtype=tf.int64) for feature in identifier_features\n",
    "]\n",
    "\n",
    "categorical_inputs = [\n",
    "    Input(shape=(1,), name=feature, dtype=tf.int64) for feature in categorical_features\n",
    "]\n",
    "\n",
    "# Process categorical inputs with embeddings\n",
    "embedded_categorical = []\n",
    "for feature, input_layer in zip(categorical_features, categorical_inputs):\n",
    "    embedding_layer = embedding_layers[feature]  # Retrieve pre-defined embedding layer\n",
    "    embedded_output = Flatten()(\n",
    "        embedding_layer(input_layer)\n",
    "    )  # Flatten (batch_size, 1, embedding_dim) -> (batch_size, embedding_dim)\n",
    "    embedded_categorical.append(embedded_output)\n",
    "\n",
    "# Concatenate all processed inputs\n",
    "# Shapes:\n",
    "# - Numerical inputs: (batch_size, 1)\n",
    "# - Identifier inputs: (batch_size, 1)\n",
    "# - Embedded categorical: (batch_size, embedding_dim)\n",
    "all_features = Concatenate(axis=-1)(\n",
    "    numerical_inputs + identifier_inputs + embedded_categorical\n",
    ")\n",
    "print(f\"Shape after concatenation: {all_features.shape}\")  # Debugging output\n",
    "\n",
    "# Define dense layers\n",
    "x = Dense(128, activation=\"relu\", name=\"dense_1\")(all_features)\n",
    "x = BatchNormalization(name=\"batch_norm_1\")(x)\n",
    "x = Dropout(0.5, name=\"dropout_1\")(x)\n",
    "\n",
    "x = Dense(64, activation=\"relu\", name=\"dense_2\")(x)\n",
    "x = BatchNormalization(name=\"batch_norm_2\")(x)\n",
    "x = Dropout(0.3, name=\"dropout_2\")(x)\n",
    "\n",
    "# Output layer with softmax activation for multi-class classification\n",
    "num_classes = len(vocab)\n",
    "output = Dense(num_classes, activation=\"softmax\", name=\"output\")(x)\n",
    "\n",
    "# Build the model\n",
    "model = Model(\n",
    "    inputs=numerical_inputs + identifier_inputs + categorical_inputs,\n",
    "    outputs=output,\n",
    "    name=\"network_traffic_classifier\",\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Compute Class Weights\n",
    "\n",
    "# Collect labels from a subset for class weight computation\n",
    "labels_subset = []\n",
    "for _, label in train_dataset.take(batches_for_adaptation):\n",
    "    labels_subset.extend(label.numpy().tolist())\n",
    "\n",
    "# Compute class weights using scikit-learn\n",
    "class_weights_array = class_weight.compute_class_weight(\n",
    "    class_weight=\"balanced\", classes=np.unique(labels_subset), y=labels_subset\n",
    ")\n",
    "\n",
    "# Convert to dictionary\n",
    "class_weights = {i: weight for i, weight in enumerate(class_weights_array)}\n",
    "print(\"Class Weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Calculate Steps Per Epoch\n",
    "\n",
    "\n",
    "def count_lines(file_path):\n",
    "    \"\"\"Counts the number of lines in a file.\"\"\"\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for i, _ in enumerate(f):\n",
    "            pass\n",
    "    return i + 1  # Total number of lines including header\n",
    "\n",
    "\n",
    "# Total number of samples (subtract header)\n",
    "# total_lines = count_lines(file_path)\n",
    "total_lines = 75987977  # Since we know the total number of lines\n",
    "total_samples = total_lines - 1\n",
    "print(f\"Total lines in CSV (including header): {total_lines}\")\n",
    "print(f\"Total samples: {total_samples}\")\n",
    "\n",
    "# Since validation_split=0.2\n",
    "steps_per_epoch = int((total_samples * 0.8) // batch_size)\n",
    "validation_steps = int((total_samples * 0.2) // batch_size)\n",
    "print(f\"Steps per epoch: {steps_per_epoch}\")\n",
    "print(f\"Validation steps: {validation_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Vocabulary:\", label_lookup.get_vocabulary())\n",
    "for features, label in train_dataset.take(1):\n",
    "    print(\"Sample labels:\", label)\n",
    "\n",
    "print(\"Label values:\", label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Train the Model\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=10,  # Adjust based on convergence\n",
    "    # steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=validation_dataset,\n",
    "    # validation_steps=validation_steps,\n",
    "    class_weight=class_weights,  # Include if handling class imbalance\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\", patience=patience, restore_best_weights=True\n",
    "        ),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=model_checkpoint_path, monitor=\"val_loss\", save_best_only=True\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Evaluate the Model\n",
    "\n",
    "# Evaluate the model on the validation dataset\n",
    "val_loss, val_accuracy = model.evaluate(validation_dataset, steps=validation_steps)\n",
    "print(f\"Validation Loss: {val_loss}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "# Plot training & validation accuracy and loss values\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Retrieve history data\n",
    "acc = history.history[\"accuracy\"]\n",
    "val_acc = history.history[\"val_accuracy\"]\n",
    "\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "\n",
    "epochs_range = range(len(acc))\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label=\"Training Accuracy\")\n",
    "plt.plot(epochs_range, val_acc, label=\"Validation Accuracy\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.title(\"Training and Validation Accuracy\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label=\"Training Loss\")\n",
    "plt.plot(epochs_range, val_loss, label=\"Validation Loss\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Save and Load the Model\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"final_model.h5\")\n",
    "print(\"Model saved to 'final_model.h5'.\")\n",
    "\n",
    "# Load the model (if needed)\n",
    "# loaded_model = tf.keras.models.load_model(\"final_model.h5\")\n",
    "# print(\"Model loaded from 'final_model.h5'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17: Make Predictions\n",
    "\n",
    "# Example: Get a batch of validation data\n",
    "for batch_features, batch_labels in validation_dataset.take(1):\n",
    "    predictions = model.predict(batch_features)\n",
    "    predicted_classes = tf.argmax(predictions, axis=1).numpy()\n",
    "    true_classes = batch_labels.numpy()\n",
    "    print(\"Predicted classes:\", predicted_classes)\n",
    "    print(\"True classes:\", true_classes)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18: Additional Evaluation Metrics\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Collect all predictions and true labels\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for batch_features, batch_labels in validation_dataset:\n",
    "    preds = model.predict(batch_features)\n",
    "    preds = tf.argmax(preds, axis=1).numpy()\n",
    "    y_pred.extend(preds)\n",
    "    y_true.extend(batch_labels.numpy())\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(\n",
    "    y_true, y_pred, target_names=vocab[1:]\n",
    ")  # Exclude '[UNK]' if present\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 19: Feature Importance and Interpretation\n",
    "\n",
    "import shap\n",
    "\n",
    "# Select a subset of the validation data for SHAP\n",
    "sample_features, sample_labels = next(iter(validation_dataset.take(1)))\n",
    "\n",
    "\n",
    "# Define a prediction function for SHAP\n",
    "def model_predict(inputs):\n",
    "    return model(inputs)\n",
    "\n",
    "\n",
    "# Initialize SHAP explainer\n",
    "explainer = shap.GradientExplainer(model_predict, sample_features)\n",
    "\n",
    "# Compute SHAP values\n",
    "shap_values = explainer.shap_values(sample_features)\n",
    "\n",
    "# Plot SHAP summary\n",
    "shap.summary_plot(\n",
    "    shap_values, sample_features, feature_names=list(sample_features.keys())\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
